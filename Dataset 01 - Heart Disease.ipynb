{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c95daf",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# üå≥ Project 2: Decision Tree Classifier\n",
    "\n",
    "\n",
    "## üë• Th√†nh vi√™n nh√≥m\n",
    "\n",
    "| üÜî MSSV    | üë®‚Äçüéì H·ªç v√† T√™n           |\n",
    "|-----------|--------------------------|\n",
    "| 22120194  | Nguy·ªÖn Nh·∫≠t Long         |\n",
    "| 22120197  | Nguy·ªÖn Vƒ©nh L∆∞∆°ng        |\n",
    "| 22120238  | Nguy·ªÖn Minh Nguy√™n       |\n",
    "| 22120252  | Giang ƒê·ª©c Nh·∫≠t           |\n",
    "\n",
    "## ‚ô•Ô∏è Dataset 01: Heart Disease\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5da50",
   "metadata": {},
   "source": [
    "## **0. Chu·∫©n b·ªã th∆∞ vi·ªán**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54edc4c",
   "metadata": {},
   "source": [
    "### **0.1 C√†i c√°c packages c·∫ßn thi·∫øt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q matplotlib\n",
    "%pip install -q scikit-learn\n",
    "%pip install -q pandas\n",
    "%pip install -q numpy\n",
    "%pip install -q graphviz\n",
    "%pip install -q ucimlrepo\n",
    "%pip install -q seaborn\n",
    "%pip install -q category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48cedf",
   "metadata": {},
   "source": [
    "### **0.2 Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "025dc041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from graphviz import Source\n",
    "from IPython.display import display, Image\n",
    "import seaborn as sns\n",
    "import os\n",
    "import category_encoders as ce\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af45b9",
   "metadata": {},
   "source": [
    "### **0.3 H√†m ti·ªÅn x·ª≠ l√Ω**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f827a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, nominal_cols=None, ordinal_cols=None, numerical_cols=None):\n",
    "    # Drop duplicates\n",
    "    dataset.data.original.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Solve missing values\n",
    "    df = dataset.data.original # features + target\n",
    "    var_info = dataset.variables # name, role, type, demographic, description, units, missing_values\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Find corresponding variable information\n",
    "        var_row = var_info[var_info['name'] == col]\n",
    "        if not var_row.empty and var_row.iloc[0]['missing_values'] == 'yes':\n",
    "            if nominal_cols and col in nominal_cols:\n",
    "                # Most frequent cho nominal\n",
    "                most_frequent = df[col].mode(dropna=True)[0]\n",
    "                df[col].fillna(most_frequent, inplace=True)\n",
    "            elif ordinal_cols and col in ordinal_cols:\n",
    "                # Most frequent cho ordinal\n",
    "                most_frequent = df[col].mode(dropna=True)[0]\n",
    "                df[col].fillna(most_frequent, inplace=True)\n",
    "            elif numerical_cols and col in numerical_cols:\n",
    "                # Median cho numeric\n",
    "                median = df[col].median(skipna=True)\n",
    "                df[col].fillna(median, inplace=True)\n",
    "\n",
    "    # One-Hot Encoding cho Nominal\n",
    "    if nominal_cols:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        ohe_arr = ohe.fit_transform(df[nominal_cols]).toarray()\n",
    "        ohe_df = pd.DataFrame(ohe_arr, columns=ohe.get_feature_names_out(nominal_cols), index=df.index)\n",
    "        df = df.drop(columns=nominal_cols)\n",
    "        df = pd.concat([df, ohe_df], axis=1)\n",
    "\n",
    "    # Ordinal Encoding cho Ordinal\n",
    "    if ordinal_cols:\n",
    "        ord_enc = OrdinalEncoder()\n",
    "        df[ordinal_cols] = ord_enc.fit_transform(df[ordinal_cols])\n",
    "\n",
    "    # Return processed DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9074a98",
   "metadata": {},
   "source": [
    "## **1. Binary class dataset: The UCI Heart Disease dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf7c98",
   "metadata": {},
   "source": [
    "### **1.1. Chu·∫©n b·ªã Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff3ff5",
   "metadata": {},
   "source": [
    "#### *1.1.1 Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7114a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = fetch_ucirepo(id=45)\n",
    "target_col_name = heart_disease.metadata.target_col\n",
    "\n",
    "# If target_col_name is a list, take the first element\n",
    "if isinstance(target_col_name, (list, tuple)) and len(target_col_name) == 1:\n",
    "    target_col_name = target_col_name[0]\n",
    "\n",
    "nominal_cols = ['sex', 'fbs', 'exang', 'thal']\n",
    "ordinal_cols = ['cp', 'restecg', 'slope']\n",
    "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "\n",
    "heart_disease_processed = preprocess_data(heart_disease, nominal_cols, ordinal_cols, numerical_cols)\n",
    "\n",
    "labels = heart_disease_processed[target_col_name]\n",
    "# Fix target dataset\n",
    "labels = labels.apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "features = heart_disease_processed.drop(columns=target_col_name)\n",
    "\n",
    "print(features)\n",
    "\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143050cd",
   "metadata": {},
   "source": [
    "#### *1.1.2 Chu·∫©n b·ªã train v√† test dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14b23342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train/test split proportions\n",
    "split_ratios = [(0.4, 0.6), (0.6, 0.4), (0.8, 0.2), (0.9, 0.1)]\n",
    "\n",
    "subsets = []\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "        features, labels, test_size=split_ratio[1], random_state=45, stratify = labels\n",
    "    )\n",
    "    \n",
    "    subsets.append({\n",
    "        'feature_train': feature_train,\n",
    "        'label_train': label_train,\n",
    "        'feature_test': feature_test,\n",
    "        'label_test': label_test\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab85b7",
   "metadata": {},
   "source": [
    "#### *1.1.3 Visualization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0e13b",
   "metadata": {},
   "source": [
    "##### a. Ph√¢n ph·ªëi c·ªßa dataset g·ªëc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save distribution plots\n",
    "os.makedirs(\"./Dataset 01/Distribution\", exist_ok=True)\n",
    "\n",
    "# Visualize the class distribution in the original dataset\n",
    "plt.figure(figsize=(10, 5))\n",
    "bins = np.arange(len(np.unique(labels)) + 1) - 0.5\n",
    "plt.hist(labels, bins=bins, color=\"green\", alpha=0.7, edgecolor=\"black\")\n",
    "plt.title(\"Class Distribution for Original Dataset\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(np.arange(len(np.unique(labels))), np.unique(labels))\n",
    "\n",
    "# Save the plots\n",
    "plt.savefig(\"./Dataset 01/Distribution/Original Distribution.png\", format='png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f77a1f",
   "metadata": {},
   "source": [
    "##### b. Ph√¢n ph·ªëi c·ªßa t·∫≠p train/test theo m·ªói t·ªâ l·ªá chia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620fcc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single figure with subplots arranged in 4 rows x 2 columns\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(14, 16))\n",
    "fig.suptitle(\"Class Distribution for Each Train/Test Split\", fontsize=16)\n",
    "\n",
    "# Get unique class labels and create bins\n",
    "class_labels = np.unique(labels)\n",
    "bins = np.arange(len(class_labels) + 1) - 0.5 \n",
    "\n",
    "# Visualize distributions for each train/test split\n",
    "for i in range(len(subsets)):\n",
    "    label_train = subsets[i]['label_train']\n",
    "    label_test = subsets[i]['label_test']\n",
    "    train_ratio = int(split_ratios[i][0] * 100)\n",
    "    test_ratio = int(split_ratios[i][1] * 100)\n",
    "\n",
    "    # Train distribution (left column)\n",
    "    axes[i, 0].hist(label_train, bins=bins, color=\"red\", alpha=0.7, edgecolor=\"black\")\n",
    "    axes[i, 0].set_title(f\"Train {train_ratio}%\")\n",
    "    axes[i, 0].set_xticks(class_labels)\n",
    "    axes[i, 0].set_xlabel(\"Classes\")\n",
    "    axes[i, 0].set_ylabel(\"Frequency\")\n",
    "    \n",
    "    # Add count annotations on bars\n",
    "    counts_train, _ = np.histogram(label_train, bins=bins)\n",
    "    max_count_train = max(counts_train) if len(counts_train) > 0 else 0\n",
    "    \n",
    "    # Set y-axis limit with extra space for labels\n",
    "    axes[i, 0].set_ylim(0, max_count_train * 1.15)  # 15% extra space\n",
    "    \n",
    "    for j, count in enumerate(counts_train):\n",
    "        if count > 0: \n",
    "            axes[i, 0].text(class_labels[j], count + max_count_train * 0.02, \n",
    "                           str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Test distribution (right column)\n",
    "    axes[i, 1].hist(label_test, bins=bins, color=\"orange\", alpha=0.7, edgecolor=\"black\")\n",
    "    axes[i, 1].set_title(f\"Test {test_ratio}%\")\n",
    "    axes[i, 1].set_xticks(class_labels)\n",
    "    axes[i, 1].set_xlabel(\"Classes\")\n",
    "    axes[i, 1].set_ylabel(\"Frequency\")\n",
    "    \n",
    "    # Add count annotations on bars\n",
    "    counts_test, _ = np.histogram(label_test, bins=bins)\n",
    "    max_count_test = max(counts_test) if len(counts_test) > 0 else 0\n",
    "    \n",
    "    # Set y-axis limit with extra space for labels\n",
    "    axes[i, 1].set_ylim(0, max_count_test * 1.15)  # 15% extra space\n",
    "    \n",
    "    for j, count in enumerate(counts_test):\n",
    "        if count > 0:\n",
    "            axes[i, 1].text(class_labels[j], count + max_count_test * 0.02, \n",
    "                           str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Save the combined plot\n",
    "plt.savefig(\"./Dataset 01/Distribution/Test and Train Distribution.png\", format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe834f",
   "metadata": {},
   "source": [
    "### 1.2 X√¢y d·ª±ng Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513cf6d3",
   "metadata": {},
   "source": [
    "#### 1.2.1 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9179024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the Decision Tree model using Entropy (Information Gain)\n",
    "models = []\n",
    "for i, subset in enumerate(subsets):\n",
    "    feature_train = subset['feature_train']\n",
    "    label_train = subset['label_train']\n",
    "    model = DecisionTreeClassifier(criterion='entropy', random_state=45)\n",
    "    model.fit(feature_train, label_train)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7988b9d",
   "metadata": {},
   "source": [
    "#### 1.2.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff99843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save the decision tree images\n",
    "os.makedirs(\"./Dataset 01/Decision Tree\", exist_ok=True)\n",
    "\n",
    "# Create and visualize decision trees for each model\n",
    "for i in range(len(models)):\n",
    "    ratio_name = f\"{int(split_ratios[i][0]*100)}_{int(split_ratios[i][1]*100)}\"\n",
    "\n",
    "    print(f\"Decision tree trained with a train/test split of {ratio_name}\")\n",
    "\n",
    "    # Get the model and feature names\n",
    "    model = models[i]\n",
    "    feature_names = features.columns.tolist()\n",
    "\n",
    "    dot_data = export_graphviz(\n",
    "        model,\n",
    "        out_file=None,\n",
    "        feature_names=feature_names,\n",
    "        class_names=['No Disease', 'Has Disease'],\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        special_characters=True\n",
    "    )\n",
    "\n",
    "    # Visualize the decision tree\n",
    "    graph = Source(dot_data)\n",
    "    file_path = f\"./Dataset 01/Decision Tree/Tree_{ratio_name}\"\n",
    "    graph.render(file_path, format='png', cleanup=True)\n",
    "    display(Image(file_path + \".png\"))\n",
    "    \n",
    "    if(i != len(subsets) - 1):\n",
    "        print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34168535",
   "metadata": {},
   "source": [
    "### 1.3 ƒê√°nh gi√° Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb89fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save classification and confusion matrices\n",
    "os.makedirs(\"./Dataset 01/Evaluation\", exist_ok=True)\n",
    "\n",
    "\n",
    "# For each model and split, make predictions, generate a report, and confusion matrix\n",
    "for i, subset in enumerate(subsets):\n",
    "    feature_train = subset['feature_train']\n",
    "    label_train = subset['label_train']\n",
    "    feature_test = subset['feature_test']\n",
    "    label_test = subset['label_test']\n",
    "    \n",
    "    # Make predictions\n",
    "    label_pred = models[i].predict(feature_test)\n",
    "    \n",
    "    # Generate classification report\n",
    "    ratio_name = f\"{int(split_ratios[i][0]*100)}_{int(split_ratios[i][1]*100)}\"\n",
    "\n",
    "    print(f\"\\nClassification Report for {ratio_name} Split:\")\n",
    "    report_text = classification_report(label_test, label_pred, target_names=['No Disease', 'Has Disease'])\n",
    "    print(report_text)\n",
    "\n",
    "    # Save classification report to a text file\n",
    "    with open(f\"./Dataset 01/Evaluation/Report_{ratio_name}.txt\", \"w\") as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(label_test, label_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=['No Disease', 'Has Disease'],\n",
    "                yticklabels=['No Disease', 'Has Disease'])\n",
    "    plt.title(f\"Confusion Matrix for {ratio_name} Split\")\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "\n",
    "     \n",
    "    # Save confusion matrix plot\n",
    "    plt.savefig(f\"./Dataset 01/Evaluation/Confusion_{ratio_name}.png\", format='png',dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    if(i != len(subsets) - 1):\n",
    "        print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed60a7dd",
   "metadata": {},
   "source": [
    "### 1.4. ƒê·ªô s√¢u (depth) v√† ƒë·ªô ch√≠nh x√°c (accuracy) c·ªßa Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36382a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create directory for save decision tree images with max depth\n",
    "os.makedirs(\"./Dataset 01/Depth and Accuracy\", exist_ok=True)\n",
    "\n",
    "# Train and evaluate Decision Tree models with varying max depths on the 80/20 split\n",
    "subset_80_20 = subsets[2]\n",
    "feature_train_80_20 = subsets[2]['feature_train']\n",
    "label_train_80_20 = subset_80_20['label_train']\n",
    "feature_test_80_20 = subset_80_20['feature_test']\n",
    "label_test_80_20 = subset_80_20['label_test']\n",
    "accuracy_scores = []\n",
    "depths = [None, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for depth in depths:\n",
    "    model = DecisionTreeClassifier(criterion='entropy', random_state=42, max_depth=depth)\n",
    "    model.fit(feature_train_80_20, label_train_80_20)\n",
    "    feature_names = features.columns.astype(str).tolist() \n",
    "    print(f\"Decision tree trained with an 80/20 split and max depth of {depth}\")\n",
    "    \n",
    "\n",
    "    dot_data = export_graphviz(\n",
    "        model,\n",
    "        out_file=None,\n",
    "        feature_names=feature_names,\n",
    "        class_names=['No Disease', 'Has Disease'],\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        special_characters=True\n",
    "    )\n",
    "    \n",
    "    # Visualize the decision tree\n",
    "    graph = Source(dot_data)\n",
    "    file_path = f\"./Dataset 01/Depth and Accuracy/Tree_Depth_{depth}\"\n",
    "    graph.render(file_path, format='png', cleanup=True)\n",
    "    display(Image(file_path + \".png\"))\n",
    "\n",
    "    pred=model.predict(feature_test_80_20)\n",
    "    accuracy = accuracy_score(label_test_80_20, pred)\n",
    "    print(f\"Accuracy for max depth {depth}: {accuracy}\")\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print('-' * 100)\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "depths_for_plot = [str(d) if d is not None else 'None' for d in depths]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(depths_for_plot, accuracy_scores, marker='o', color='blue')\n",
    "plt.title('Accuracy vs Max Depth for Decision Tree')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "y_min = min(accuracy_scores)\n",
    "y_max = max(accuracy_scores)\n",
    "y_range = y_max - y_min\n",
    "padding = y_range * 0.1  # 5% of the value range\n",
    "plt.ylim(y_min - padding, y_max + padding)\n",
    "plt.xticks(depths_for_plot)\n",
    "plt.grid(True)\n",
    "\n",
    "for i, (x, y) in enumerate(zip(depths_for_plot, accuracy_scores)):\n",
    "    plt.text(x, y + 0.003, f\"{y:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.savefig(f\"./Dataset 01/Depth and Accuracy/Chart Statistics.png\", format='png',dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print table\n",
    "table_data = {\n",
    "    depth if depth is not None else 'None': [f\"{acc:.3f}\"]\n",
    "    for depth, acc in zip(depths_for_plot, accuracy_scores)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(table_data, index=[\"Accuracy\"])\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
